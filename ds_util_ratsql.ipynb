{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254861f4249441179adbf776260b40ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping writing to data cache since max_instances was specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7904ebf148b442f4a0932b9ebdc19d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[“CUDA_DEVICE_ORDER”]=“PCI_BUS_ID”\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# experiment_name = \"3_heads_lr3_keep_op_identity+agenda_enriched_all+lr_e3+mult_scalar_per_action+glove\"\n",
    "# experiment_name = \"crappy-red-dhole\"\n",
    "\n",
    "from allennlp.models.archival import Archive\n",
    "import torch\n",
    "from allennlp.common import Params\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.common.params import with_fallback\n",
    "from dataset_readers.spider_ratsql import SpiderRatsqlDatasetReader\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "\n",
    "# from models.semantic_parsing.spider_parser import SpiderParser\n",
    "reader = SpiderRatsqlDatasetReader(tables_file=\"dataset/tables.json\",max_instances=10)\n",
    "# settings = Params.from_file(f\"experiments/{experiment_name}/config.json\")\n",
    "# model = Model.load(config=settings, serialization_dir=f\"experiments/{experiment_name}\")\n",
    "\n",
    "\n",
    "train_dataset = reader.read(\"dataset/train_spider.json\")\n",
    "vocab = Vocabulary.from_instances(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = reader.read(\"dataset/train_spider.json\")\n",
    "from models.semantic_parsing.ratsql_encoder import RatsqlEncoder\n",
    "# from models.semantic_parsing.gnn_encoder import GnnEncoder\n",
    "from models.semantic_parsing.spider_decoder import SpiderParser\n",
    "from allennlp.modules.seq2vec_encoders.boe_encoder import BagOfEmbeddingsEncoder\n",
    "\n",
    "from allennlp.modules.attention import DotProductAttention\n",
    "from allennlp.nn.beam_search import BeamSearch\n",
    "from allennlp.modules.seq2seq_encoders.pass_through_encoder import PassThroughEncoder\n",
    "\n",
    "\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "# from allennlp.data.iterators import BasicIterator\n",
    "import torch.optim as optim\n",
    "from allennlp.training.trainer import Trainer\n",
    "import torch\n",
    "# with torch.cuda.device(0):\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 200\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "beam = BeamSearch(end_index=0,beam_size=10)\n",
    "\n",
    "schema_encoder = RatsqlEncoder(encoder=PassThroughEncoder(200),entity_encoder=BagOfEmbeddingsEncoder(200),question_embedder=word_embeddings,action_embedding_dim=200)\n",
    "# schema_encoder = GnnEncoder(encoder=PassThroughEncoder(200),entity_encoder=BagOfEmbeddingsEncoder(200),question_embedder=word_embeddings,action_embedding_dim=200)\n",
    "model = SpiderParser(vocab=vocab,schema_encoder=schema_encoder, \n",
    "                     decoder_beam_search=beam,input_attention=DotProductAttention(),past_attention=DotProductAttention(),max_decoding_steps=10)\n",
    "\n",
    "def get_offsets(lengths):\n",
    "    e = np.cumsum(([0]+list(lengths))[:-1])\n",
    "    return list(zip(e+1,e+np.array(lengths)))\n",
    "\n",
    "#     # model.cuda()\n",
    "# iterator = BasicIterator(batch_size=15)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# iterator.index_with(vocab)\n",
    "# trainer = Trainer(model=model,\n",
    "#                   optimizer=optimizer,\n",
    "#                   iterator=iterator,\n",
    "#                   train_dataset=train_dataset,\n",
    "#                   validation_dataset=train_dataset,\n",
    "#                   patience=10,\n",
    "#                   num_epochs=1)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fb3e0cffbd57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mres_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_on_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mforward_on_instances\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_output_human_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             instance_separated_output: List[Dict[str, numpy.ndarray]] = [\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/Courses/2019B/Research/spider-schema-gnn-global/models/semantic_parsing/spider_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, valid_actions, world, utterance, schema, action_sequence, enc, relation, schema_strings, lengths, offsets)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 decode_output = self._decoder_trainer.decode(initial_state,\n\u001b[0m\u001b[1;32m    150\u001b[0m                                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transition_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                                                              (action_sequence.unsqueeze(1), action_mask.unsqueeze(1)))\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/allennlp_semparse/state_machines/trainers/maximum_marginal_likelihood.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, initial_state, transition_function, supervision)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupervision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mbeam_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConstrainedBeamSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_beam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         finished_states: Dict[int, List[State]] = beam_search.search(\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransition_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         )\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/allennlp_semparse/state_machines/constrained_beam_search.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, initial_state, transition_function)\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allowed_transitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 )\n\u001b[0;32m---> 97\u001b[0;31m             for next_state in transition_function.take_step(\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0mgrouped_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_per_node_beam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed_actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             ):\n",
      "\u001b[0;32m~/workspace/Courses/2019B/Research/spider-schema-gnn-global/state_machines/transition_functions/attend_past_schema_items_transition.py\u001b[0m in \u001b[0;36mtake_step\u001b[0;34m(self, state, max_actions, allowed_actions)\u001b[0m\n\u001b[1;32m     64\u001b[0m                                                            \u001b[0mupdated_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'past_schema_items_attention_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                                                            updated_state['predicted_action_embeddings'])\n\u001b[0;32m---> 66\u001b[0;31m         new_states = self._construct_next_states(state,\n\u001b[0m\u001b[1;32m     67\u001b[0m                                                  \u001b[0mupdated_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                                  \u001b[0mbatch_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/Courses/2019B/Research/spider-schema-gnn-global/state_machines/transition_functions/attend_past_schema_items_transition.py\u001b[0m in \u001b[0;36m_construct_next_states\u001b[0;34m(self, state, updated_rnn_state, batch_action_probs, max_actions, allowed_actions)\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;31m# If we're given a set of allowed actions, and we're not just keeping the top k of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0;31m# them, we don't need to do any sorting, so we can speed things up quite a bit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mgroup_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "source": [
    "a=None\n",
    "for inst in train_dataset:\n",
    "    a = inst\n",
    "    res_list = model.forward_on_instances([inst])\n",
    "    break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list()\n",
    "import numpy as np\n",
    "b = a.fields['enc']\n",
    "token_ids = [0]+b.tokens\n",
    "c = a.fields['lengths'].array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0]+token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2129"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = b.tokens[0]\n",
    "# vars(y)\n",
    "y.text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2129]\n",
      "[how]\n",
      "[2116]\n",
      "[many]\n",
      "[4641]\n",
      "[heads]\n",
      "[1997]\n",
      "[of]\n",
      "[1996]\n",
      "[the]\n",
      "[7640]\n",
      "[departments]\n",
      "[2024]\n",
      "[are]\n",
      "[3080]\n",
      "[older]\n",
      "[2084]\n",
      "[than]\n",
      "[5179]\n",
      "[56]\n",
      "[1029]\n",
      "[?]\n",
      "[1026, 2828, 1024, 3793, 1028, 1008, 1026, 2795, 1011, 19802, 1028, 1026, 2151, 1011, 2795, 1028]\n",
      "[<, type, :, text, >, *, <, table, -, sep, >, <, any, -, table, >]\n",
      "[1026, 2828, 1024, 2193, 1028, 2533, 8909, 1026, 2795, 1011, 19802, 1028, 2533]\n",
      "[<, type, :, number, >, department, id, <, table, -, sep, >, department]\n",
      "[1026, 2828, 1024, 3793, 1028, 2171, 1026, 2795, 1011, 19802, 1028, 2533]\n",
      "[<, type, :, text, >, name, <, table, -, sep, >, department]\n",
      "[1026, 2828, 1024, 3793, 1028, 4325, 1026, 2795, 1011, 19802, 1028, 2533]\n",
      "[<, type, :, text, >, creation, <, table, -, sep, >, department]\n",
      "[1026, 2828, 1024, 2193, 1028, 5464, 1026, 2795, 1011, 19802, 1028, 2533]\n",
      "[<, type, :, number, >, ranking, <, table, -, sep, >, department]\n",
      "[1026, 2828, 1024, 2193, 1028, 5166, 1999, 25501, 1026, 2795, 1011, 19802, 1028, 2533]\n",
      "[<, type, :, number, >, budget, in, billions, <, table, -, sep, >, department]\n",
      "[1026, 2828, 1024, 2193, 1028, 16371, 2213, 5126, 1026, 2795, 1011, 19802, 1028, 2533]\n",
      "[<, type, :, number, >, nu, ##m, employees, <, table, -, sep, >, department]\n",
      "[1026, 2828, 1024, 2193, 1028, 2132, 8909, 1026, 2795, 1011, 19802, 1028, 2132]\n",
      "[<, type, :, number, >, head, id, <, table, -, sep, >, head]\n",
      "[1026, 2828, 1024, 3793, 1028, 2171, 1026, 2795, 1011, 19802, 1028, 2132]\n",
      "[<, type, :, text, >, name, <, table, -, sep, >, head]\n",
      "[1026, 2828, 1024, 3793, 1028, 2141, 2110, 1026, 2795, 1011, 19802, 1028, 2132]\n",
      "[<, type, :, text, >, born, state, <, table, -, sep, >, head]\n",
      "[1026, 2828, 1024, 2193, 1028, 2287, 1026, 2795, 1011, 19802, 1028, 2132]\n",
      "[<, type, :, number, >, age, <, table, -, sep, >, head]\n",
      "[1026, 2828, 1024, 2193, 1028, 2533, 8909, 1026, 2795, 1011, 19802, 1028, 2968]\n",
      "[<, type, :, number, >, department, id, <, table, -, sep, >, management]\n",
      "[1026, 2828, 1024, 2193, 1028, 2132, 8909, 1026, 2795, 1011, 19802, 1028, 2968]\n",
      "[<, type, :, number, >, head, id, <, table, -, sep, >, management]\n",
      "[1026, 2828, 1024, 3793, 1028, 5741, 3772, 1026, 2795, 1011, 19802, 1028, 2968]\n",
      "[<, type, :, text, >, temporary, acting, <, table, -, sep, >, management]\n",
      "[1026, 2795, 1028, 2533]\n",
      "[<, table, >, department]\n",
      "[1026, 2795, 1028, 2132]\n",
      "[<, table, >, head]\n",
      "[1026, 2795, 1028, 2968]\n",
      "[<, table, >, management]\n"
     ]
    }
   ],
   "source": [
    "# print(c.array)\n",
    "offsets = get_offsets(c)\n",
    "res = [token_ids[offsets[j][0]:offsets[j][1] + 1] for j in range(len(offsets))]\n",
    "# print(res)\n",
    "# print()\n",
    "# for x in [[vocab.get_token_from_index(y,\"tags\") for y in x] for x in res]:\n",
    "for x in res:\n",
    "    print([y.text_id for y in x])\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-bcc9a13cbb44>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-bcc9a13cbb44>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    13, 12, 13, 12, 13, 13, 13,  4,  4,  4]], dtype=torch.int32) torch.Size([1, 28])\u001b[0m\n\u001b[0m                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "torch.Size([1, 205, 2])\n",
    "torch.Size([1, 205, 768])\n",
    "tensor([[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 16, 13, 12, 12, 12, 14, 14,\n",
    "         13, 12, 13, 12, 13, 13, 13,  4,  4,  4]], dtype=torch.int32) torch.Size([1, 28])\n",
    "tensor([2129])\n",
    "tensor([2116])\n",
    "tensor([4641])\n",
    "tensor([1997])\n",
    "tensor([1996])\n",
    "tensor([7640])\n",
    "tensor([2024])\n",
    "tensor([3080])\n",
    "tensor([2084])\n",
    "tensor([5179])\n",
    "tensor([1029])\n",
    "tensor([ 1026,  2828,  1024,  3793,  1028,  1008,  1026,  2795,  1011, 19802,\n",
    "         1028,  1026,  2151,  1011,  2795,  1028])\n",
    "tensor([ 1026,  2828,  1024,  2193,  1028,  2533,  8909,  1026,  2795,  1011,\n",
    "        19802,  1028,  2533])\n",
    "tensor([ 1026,  2828,  1024,  3793,  1028,  2171,  1026,  2795,  1011, 19802,\n",
    "         1028,  2533])\n",
    "tensor([ 1026,  2828,  1024,  3793,  1028,  4325,  1026,  2795,  1011, 19802,\n",
    "         1028,  2533])\n",
    "tensor([ 1026,  2828,  1024,  2193,  1028,  5464,  1026,  2795,  1011, 19802,\n",
    "         1028,  2533])\n",
    "tensor([ 1026,  2828,  1024,  2193,  1028,  5166,  1999, 25501,  1026,  2795,\n",
    "         1011, 19802,  1028,  2533])\n",
    "tensor([ 1026,  2828,  1024,  2193,  1028, 16371,  1001,  1001,  1049,  5126,\n",
    "         1026,  2795,  1011, 19802])\n",
    "tensor([ 1028,  2533,  1026,  2828,  1024,  2193,  1028,  2132,  8909,  1026,\n",
    "         2795,  1011, 19802])\n",
    "tensor([ 1028,  2132,  1026,  2828,  1024,  3793,  1028,  2171,  1026,  2795,\n",
    "         1011, 19802])\n",
    "tensor([ 1028,  2132,  1026,  2828,  1024,  3793,  1028,  2141,  2110,  1026,\n",
    "         2795,  1011, 19802])\n",
    "tensor([ 1028,  2132,  1026,  2828,  1024,  2193,  1028,  2287,  1026,  2795,\n",
    "         1011, 19802])\n",
    "tensor([ 1028,  2132,  1026,  2828,  1024,  2193,  1028,  2533,  8909,  1026,\n",
    "         2795,  1011, 19802])\n",
    "tensor([ 1028,  2968,  1026,  2828,  1024,  2193,  1028,  2132,  8909,  1026,\n",
    "         2795,  1011, 19802])\n",
    "tensor([ 1028,  2968,  1026,  2828,  1024,  3793,  1028,  5741,  3772,  1026,\n",
    "         2795,  1011, 19802])\n",
    "tensor([1028, 2968, 1026, 2795])\n",
    "tensor([1028, 2533, 1026, 2795])\n",
    "tensor([1028, 2132, 1026, 2795])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=None\n",
    "for inst in train_dataset:\n",
    "    a = inst\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773bebcaf1c548ecbb56abb1a4bbc254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset)\n",
    "b=a.fields['enc']\n",
    "b.index(vocab)\n",
    "tok_dict = b._indexed_tokens['tokens']\n",
    "print(len(tok_dict['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "c=a.fields['lengths'].array\n",
    "token_ids = tok_dict['token_ids']\n",
    "old_offsets = tok_dict['offsets']\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "to_str = lambda x: [vocab.get_token_from_index(y,\"tags\") for y in x]\n",
    "lengths = np.array(list(c))+1\n",
    "# offsets = np.cumsum(lengths-1)\n",
    "# offsets\n",
    "# print(lengths)\n",
    "\n",
    "# offsets = list(zip(offsets,offsets)\n",
    "# offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# a_ = \n",
    "# b_=offsets+lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_offsets(lengths):\n",
    "    e = np.cumsum(([0]+list(lengths))[:-1])\n",
    "    return list(zip(e+1,e+np.array(lengths)))\n",
    "\n",
    "offsets = get_offsets(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1),\n",
       " (2, 2),\n",
       " (3, 3),\n",
       " (4, 4),\n",
       " (5, 5),\n",
       " (6, 6),\n",
       " (7, 7),\n",
       " (8, 8),\n",
       " (9, 9),\n",
       " (10, 10),\n",
       " (11, 11),\n",
       " (12, 27),\n",
       " (28, 40),\n",
       " (41, 52),\n",
       " (53, 64),\n",
       " (65, 76),\n",
       " (77, 90),\n",
       " (91, 104),\n",
       " (105, 117),\n",
       " (118, 129),\n",
       " (130, 142),\n",
       " (143, 154),\n",
       " (155, 167),\n",
       " (168, 180),\n",
       " (181, 193),\n",
       " (194, 197),\n",
       " (198, 201),\n",
       " (202, 205)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how']\n",
      "['many']\n",
      "['heads']\n",
      "['of']\n",
      "['the']\n",
      "['departments']\n",
      "['are']\n",
      "['older']\n",
      "['than']\n",
      "['56']\n",
      "['?']\n",
      "['<', 'type', ':', 'text', '>', '*', '<', 'table', '-', 'sep', '>', '<', 'any', '-', 'table', '>']\n",
      "['<', 'type', ':', 'number', '>', 'department', 'id', '<', 'table', '-', 'sep', '>', 'department']\n",
      "['<', 'type', ':', 'text', '>', 'name', '<', 'table', '-', 'sep', '>', 'department']\n",
      "['<', 'type', ':', 'text', '>', 'creation', '<', 'table', '-', 'sep', '>', 'department']\n",
      "['<', 'type', ':', 'number', '>', 'ranking', '<', 'table', '-', 'sep', '>', 'department']\n",
      "['<', 'type', ':', 'number', '>', 'budget', 'in', 'billions', '<', 'table', '-', 'sep', '>', 'department']\n",
      "['<', 'type', ':', 'number', '>', 'nu', '#', '#', 'm', 'employees', '<', 'table', '-', 'sep']\n",
      "['>', 'department', '<', 'type', ':', 'number', '>', 'head', 'id', '<', 'table', '-', 'sep']\n",
      "['>', 'head', '<', 'type', ':', 'text', '>', 'name', '<', 'table', '-', 'sep']\n",
      "['>', 'head', '<', 'type', ':', 'text', '>', 'born', 'state', '<', 'table', '-', 'sep']\n",
      "['>', 'head', '<', 'type', ':', 'number', '>', 'age', '<', 'table', '-', 'sep']\n",
      "['>', 'head', '<', 'type', ':', 'number', '>', 'department', 'id', '<', 'table', '-', 'sep']\n",
      "['>', 'management', '<', 'type', ':', 'number', '>', 'head', 'id', '<', 'table', '-', 'sep']\n",
      "['>', 'management', '<', 'type', ':', 'text', '>', 'temporary', 'acting', '<', 'table', '-', 'sep']\n",
      "['>', 'management', '<', 'table']\n",
      "['>', 'department', '<', 'table']\n",
      "['>', 'head', '<', 'table']\n"
     ]
    }
   ],
   "source": [
    "# _token_indexers\n",
    "# to_str = lambda x: vocab.get_token_from_index(x,\"tags\")\n",
    "# print(c.array)\n",
    "res = [token_ids[offsets[j][0]:offsets[j][1] + 1] for j in range(len(offsets))]\n",
    "# print(res)\n",
    "# print()\n",
    "for x in [[vocab.get_token_from_index(y,\"tags\") for y in x] for x in res]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary with namespaces:  rule_labels, Size: 112 || tokens, Size: 2 || Non Padded Namespaces: {'*labels', '*tags'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_token_from_index(102,'tags')\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<allennlp.data.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer at 0x7fac34243790>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d= b._token_indexers['tokens']\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-5c66adda392c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/allennlp/data/token_indexers/token_indexer.py\u001b[0m in \u001b[0;36mindices_to_tokens\u001b[0;34m(self, indexed_tokens, vocabulary)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0minto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \"\"\"\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_empty_token_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIndexedTokenList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d.indices_to_tokens([23],vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 205, 2])\n",
      "torch.Size([1, 205, 768])\n",
      "tensor([[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 16, 13, 12, 12, 12, 14, 14,\n",
      "         13, 12, 13, 12, 13, 13, 13,  4,  4,  4]], dtype=torch.int32) torch.Size([1, 28])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-78e6f45fe5f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_on_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/allennlp/models/model.py\u001b[0m in \u001b[0;36mforward_on_instances\u001b[0;34m(self, instances)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmodel_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_output_human_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             instance_separated_output: List[Dict[str, numpy.ndarray]] = [\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/Courses/2019B/Research/spider-schema-gnn-global/models/semantic_parsing/spider_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, valid_actions, world, utterance, schema, action_sequence, enc, relation, schema_strings, lengths)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 decode_output = self._decoder_trainer.decode(initial_state,\n\u001b[0m\u001b[1;32m    147\u001b[0m                                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transition_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                                                              (action_sequence.unsqueeze(1), action_mask.unsqueeze(1)))\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/allennlp_semparse/state_machines/trainers/maximum_marginal_likelihood.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, initial_state, transition_function, supervision)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupervision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mbeam_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConstrainedBeamSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_beam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         finished_states: Dict[int, List[State]] = beam_search.search(\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransition_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         )\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.0/lib/python3.8/site-packages/allennlp_semparse/state_machines/constrained_beam_search.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, initial_state, transition_function)\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allowed_transitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 )\n\u001b[0;32m---> 97\u001b[0;31m             for next_state in transition_function.take_step(\n\u001b[0m\u001b[1;32m     98\u001b[0m                 \u001b[0mgrouped_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_per_node_beam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallowed_actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             ):\n",
      "\u001b[0;32m~/workspace/Courses/2019B/Research/spider-schema-gnn-global/state_machines/transition_functions/attend_past_schema_items_transition.py\u001b[0m in \u001b[0;36mtake_step\u001b[0;34m(self, state, max_actions, allowed_actions)\u001b[0m\n\u001b[1;32m     64\u001b[0m                                                            \u001b[0mupdated_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'past_schema_items_attention_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                                                            updated_state['predicted_action_embeddings'])\n\u001b[0;32m---> 66\u001b[0;31m         new_states = self._construct_next_states(state,\n\u001b[0m\u001b[1;32m     67\u001b[0m                                                  \u001b[0mupdated_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                                                  \u001b[0mbatch_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/Courses/2019B/Research/spider-schema-gnn-global/state_machines/transition_functions/attend_past_schema_items_transition.py\u001b[0m in \u001b[0;36m_construct_next_states\u001b[0;34m(self, state, updated_rnn_state, batch_action_probs, max_actions, allowed_actions)\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;31m# If we're given a set of allowed actions, and we're not just keeping the top k of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0;31m# them, we don't need to do any sorting, so we can speed things up quite a bit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mgroup_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_actions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "source": [
    "model.forward_on_instances([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "i=random.randint(0,len(train_dataset))\n",
    "a=None\n",
    "for inst in train_dataset[i:i+1]:\n",
    "    a = inst\n",
    "    res_list = model.forward_on_instances([inst])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_question': 'What are the names of the states where at least 3 heads were born?',\n",
       " 'question': ['What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'names',\n",
       "  'of',\n",
       "  'the',\n",
       "  'states',\n",
       "  'where',\n",
       "  'at',\n",
       "  'least',\n",
       "  '3',\n",
       "  'heads',\n",
       "  'were',\n",
       "  'born',\n",
       "  '?'],\n",
       " 'question_for_copying': ['What',\n",
       "  'are',\n",
       "  'the',\n",
       "  'names',\n",
       "  'of',\n",
       "  'the',\n",
       "  'states',\n",
       "  'where',\n",
       "  'at',\n",
       "  'least',\n",
       "  '3',\n",
       "  'heads',\n",
       "  'were',\n",
       "  'born',\n",
       "  '?'],\n",
       " 'db_id': 'department_management',\n",
       " 'sc_link': {'q_col_match': {'13,9': 'CPM'}, 'q_tab_match': {}},\n",
       " 'cv_link': {'num_date_match': {'10,1': 'NUMBER',\n",
       "   '10,4': 'NUMBER',\n",
       "   '10,5': 'NUMBER',\n",
       "   '10,6': 'NUMBER',\n",
       "   '10,7': 'NUMBER',\n",
       "   '10,10': 'NUMBER',\n",
       "   '10,11': 'NUMBER',\n",
       "   '10,12': 'NUMBER'},\n",
       "  'cell_match': {}},\n",
       " 'columns': [['<type: text>', '*', '<table-sep>', '<any-table>'],\n",
       "  ['<type: number>', 'department', 'id', '<table-sep>', 'department'],\n",
       "  ['<type: text>', 'name', '<table-sep>', 'department'],\n",
       "  ['<type: text>', 'creation', '<table-sep>', 'department'],\n",
       "  ['<type: number>', 'ranking', '<table-sep>', 'department'],\n",
       "  ['<type: number>', 'budget', 'in', 'billions', '<table-sep>', 'department'],\n",
       "  ['<type: number>', 'num', 'employees', '<table-sep>', 'department'],\n",
       "  ['<type: number>', 'head', 'id', '<table-sep>', 'head'],\n",
       "  ['<type: text>', 'name', '<table-sep>', 'head'],\n",
       "  ['<type: text>', 'born', 'state', '<table-sep>', 'head'],\n",
       "  ['<type: number>', 'age', '<table-sep>', 'head'],\n",
       "  ['<type: number>', 'department', 'id', '<table-sep>', 'management'],\n",
       "  ['<type: number>', 'head', 'id', '<table-sep>', 'management'],\n",
       "  ['<type: text>', 'temporary', 'acting', '<table-sep>', 'management']],\n",
       " 'tables': [['department'], ['head'], ['management']],\n",
       " 'table_bounds': [1, 7, 11, 14],\n",
       " 'column_to_table': {'0': None,\n",
       "  '1': 0,\n",
       "  '2': 0,\n",
       "  '3': 0,\n",
       "  '4': 0,\n",
       "  '5': 0,\n",
       "  '6': 0,\n",
       "  '7': 1,\n",
       "  '8': 1,\n",
       "  '9': 1,\n",
       "  '10': 1,\n",
       "  '11': 2,\n",
       "  '12': 2,\n",
       "  '13': 2},\n",
       " 'table_to_columns': {'0': [1, 2, 3, 4, 5, 6],\n",
       "  '1': [7, 8, 9, 10],\n",
       "  '2': [11, 12, 13]},\n",
       " 'foreign_keys': {'11': 1, '12': 7},\n",
       " 'foreign_keys_tables': {'2': [0, 1]},\n",
       " 'primary_keys': [11, 11, 11]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.fields['desc'].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= res_list[0]['initial_state']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= b.get_valid_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.get_valid_actions()\n",
    "b.possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column:foreign:management:department_id\n",
      "column:foreign:management:head_id\n",
      "column:number:department:budget_in_billions\n",
      "column:number:department:num_employees\n",
      "column:number:department:ranking\n",
      "column:number:head:age\n",
      "column:primary:department:department_id\n",
      "column:primary:head:head_id\n",
      "column:text:department:creation\n",
      "column:text:department:name\n",
      "column:text:head:born_state\n",
      "column:text:head:name\n",
      "column:text:management:temporary_acting\n",
      "string:california\n",
      "string:state\n",
      "table:department\n",
      "table:head\n",
      "table:management\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "i=random.randint(0,len(train_dataset))\n",
    "a=None\n",
    "for inst in train_dataset[i:i+1]:\n",
    "#     print(inst)\n",
    "    a=inst\n",
    "# b=a.fields['relation']\n",
    "# b.array\n",
    "b=a.fields['world']\n",
    "c=a.fields['desc']\n",
    "d=a.fields['item'] \n",
    "e = a.fields[\"schema\"]\n",
    "# b\n",
    "# print(a.fields['utterance'])\n",
    "for entity in b.metadata.db_context.knowledge_graph.entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<type: text>', '*', '<table-sep>', '<any-table>']\n",
      "<any-table>@*\n",
      "['<type: number>', 'department', 'id', '<table-sep>', 'department']\n",
      "department@department_id\n",
      "['<type: text>', 'name', '<table-sep>', 'department']\n",
      "department@name\n",
      "['<type: text>', 'creation', '<table-sep>', 'department']\n",
      "department@creation\n",
      "['<type: number>', 'ranking', '<table-sep>', 'department']\n",
      "department@ranking\n",
      "['<type: number>', 'budget', 'in', 'billions', '<table-sep>', 'department']\n",
      "department@budget_in_billions\n",
      "['<type: number>', 'num', 'employees', '<table-sep>', 'department']\n",
      "department@num_employees\n",
      "['<type: number>', 'head', 'id', '<table-sep>', 'head']\n",
      "head@head_id\n",
      "['<type: text>', 'name', '<table-sep>', 'head']\n",
      "head@name\n",
      "['<type: text>', 'born', 'state', '<table-sep>', 'head']\n",
      "head@born_state\n",
      "['<type: number>', 'age', '<table-sep>', 'head']\n",
      "head@age\n",
      "['<type: number>', 'department', 'id', '<table-sep>', 'management']\n",
      "management@department_id\n",
      "['<type: number>', 'head', 'id', '<table-sep>', 'management']\n",
      "management@head_id\n",
      "['<type: text>', 'temporary', 'acting', '<table-sep>', 'management']\n",
      "management@temporary_acting\n",
      "['department']\n",
      "department\n",
      "['head']\n",
      "head\n",
      "['management']\n",
      "management\n"
     ]
    }
   ],
   "source": [
    "for entity in (c.metadata['columns']+c.metadata['tables']):\n",
    "def normalize_schema_constant(entity):\n",
    "#     print(entity)\n",
    "    col = \"_\".join(entity)\n",
    "    col =  col.split(\"_<table-sep>_\")\n",
    "    if len(col)==1:\n",
    "        return \"_\".join(entity)\n",
    "#         print()\n",
    "    else:\n",
    "        table = col[1]\n",
    "        col = col[0].split(\">_\")[1]\n",
    "        return f\"{table}@{col}\"\n",
    "#         print()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[department, id],\n",
       " [head, id],\n",
       " [budget, in, billions],\n",
       " [num, employees],\n",
       " [ranking],\n",
       " [age],\n",
       " [department, id],\n",
       " [head, id],\n",
       " [creation],\n",
       " [name],\n",
       " [born, state],\n",
       " [name],\n",
       " [temporary, acting],\n",
       " [rank],\n",
       " [department],\n",
       " [head],\n",
       " [management]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.entity_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListField of 144 ProductionRuleFields : \n",
      " \t ProductionRuleField with rule: arg_list -> [expr, \",\", arg_list] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: arg_list -> [expr] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: arg_list_or_star -> [\"*\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: arg_list_or_star -> [arg_list] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: binaryop -> [\"!=\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: binaryop -> [\"*\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: binaryop -> [\"+\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: binaryop -> [\"-\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: binaryop -> [\"/\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: binaryop -> [\"<\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: binaryop -> [\"<=\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: binaryop -> [\"<>\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: binaryop -> [\"=\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: binaryop -> [\">\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: binaryop -> [\">=\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: binaryop -> [\"and\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: binaryop -> [\"like\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: binaryop -> [\"or\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: boolean -> [\"false\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: boolean -> [\"true\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: col_ref -> [\"department@budget_in_billions\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: col_ref -> [\"department@creation\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: col_ref -> [\"department@department_id\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: col_ref -> [\"department@name\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: col_ref -> [\"department@num_employees\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: col_ref -> [\"department@ranking\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: col_ref -> [\"head@age\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: col_ref -> [\"head@born_state\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: col_ref -> [\"head@head_id\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: col_ref -> [\"head@name\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: col_ref -> [\"management@department_id\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: col_ref -> [\"management@head_id\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: col_ref -> [\"management@temporary_acting\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: column_name -> [\"department@budget_in_billions\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: column_name -> [\"department@creation\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: column_name -> [\"department@department_id\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: column_name -> [\"department@name\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: column_name -> [\"department@num_employees\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: column_name -> [\"department@ranking\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: column_name -> [\"head@age\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: column_name -> [\"head@born_state\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: column_name -> [\"head@head_id\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: column_name -> [\"head@name\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: column_name -> [\"management@department_id\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: column_name -> [\"management@head_id\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: column_name -> [\"management@temporary_acting\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: expr -> [in_expr] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: expr -> [source_subq] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: expr -> [unaryop, expr] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: expr -> [value, \"between\", value, \"and\", value] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: expr -> [value, \"like\", string] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: expr -> [value, binaryop, expr] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: expr -> [value] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: fname -> [\"all\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: fname -> [\"avg\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: fname -> [\"count\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: fname -> [\"max\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: fname -> [\"min\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: fname -> [\"sum\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: from_clause -> [\"from\", source] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: from_clause -> [\"from\", table_name, join_clauses] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: function -> [fname, \"(\", \"distinct\", arg_list_or_star, \")\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: function -> [fname, \"(\", arg_list_or_star, \")\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: group_clause -> [expr, \",\", group_clause] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: group_clause -> [expr] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: groupby_clause -> [\"group\", \"by\", group_clause, \"having\", expr] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: groupby_clause -> [\"group\", \"by\", group_clause] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: in_expr -> [value, \"in\", expr] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: in_expr -> [value, \"in\", string_set] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: in_expr -> [value, \"not\", \"in\", expr] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: in_expr -> [value, \"not\", \"in\", string_set] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: iue -> [\"except\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: iue -> [\"intersect\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: iue -> [\"union\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: join_clause -> [\"join\", table_name, \"on\", join_condition_clause] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: join_clauses -> [join_clause, join_clauses] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: join_clauses -> [join_clause] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: join_condition -> [column_name, \"=\", column_name] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: join_condition_clause -> [join_condition, \"and\", join_condition_clause] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: join_condition_clause -> [join_condition] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: limit -> [\"limit\", non_literal_number] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: non_literal_number -> [\"1\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: non_literal_number -> [\"2\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: non_literal_number -> [\"3\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: non_literal_number -> [\"4\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: number -> [\"value\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: order_clause -> [ordering_term, \",\", order_clause] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: order_clause -> [ordering_term] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: orderby_clause -> [\"order\", \"by\", order_clause] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: ordering -> [\"asc\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: ordering -> [\"desc\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: ordering_term -> [expr, ordering] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: ordering_term -> [expr] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: parenval -> [\"(\", expr, \")\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: query -> [select_core, groupby_clause, limit] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: query -> [select_core, groupby_clause, orderby_clause, limit] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: query -> [select_core, groupby_clause, orderby_clause] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: query -> [select_core, groupby_clause] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: query -> [select_core, orderby_clause, limit] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: query -> [select_core, orderby_clause] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: query -> [select_core] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: select_core -> [select_with_distinct, select_results, from_clause, where_clause] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: select_core -> [select_with_distinct, select_results, from_clause] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: select_core -> [select_with_distinct, select_results, where_clause] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: select_core -> [select_with_distinct, select_results] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: select_result -> [\"*\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: select_result -> [column_name] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: select_result -> [expr] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: select_result -> [table_name, \".*\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: select_results -> [select_result, \",\", select_results] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: select_results -> [select_result] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: select_with_distinct -> [\"select\", \"distinct\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: select_with_distinct -> [\"select\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: single_source -> [source_subq] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: single_source -> [table_name] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: source -> [single_source, \",\", source] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: source -> [single_source] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: source_subq -> [\"(\", query, \")\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: statement -> [query, iue, query] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: statement -> [query] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: string -> [\"'\", \"value\", \"'\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: string_set -> [\"(\", string_set_vals, \")\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: string_set_vals -> [string, \",\", string_set_vals] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: string_set_vals -> [string] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: table_name -> [\"department\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: table_name -> [\"head\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: table_name -> [\"management\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: table_source -> [\"department\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: table_source -> [\"head\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: table_source -> [\"management\"] (is_global_rule: False) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: unaryop -> [\"+\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: unaryop -> [\"-\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: unaryop -> [\"not\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: value -> [\"YEAR(CURDATE())\"] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: value -> [boolean] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: value -> [column_name] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: value -> [function] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: value -> [number] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: value -> [parenval] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: value -> [string] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: where_clause -> [\"where\", expr, where_conj] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: where_clause -> [\"where\", expr] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: where_conj -> [\"and\", expr, where_conj] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      " \t ProductionRuleField with rule: where_conj -> [\"and\", expr] (is_global_rule: True) in namespace: 'rule_labels'.' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a.fields['valid_actions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column(id=3, table=Table(id=0, name=['city'], unsplit_name='city', orig_name='city', columns=[Column(id=1, table=..., name=['city', 'id'], unsplit_name='city id', orig_name='City_ID', type='number', foreign_key_for=None), Column(id=2, table=..., name=['official', 'name'], unsplit_name='official name', orig_name='Official_Name', type='text', foreign_key_for=None), ..., Column(id=4, table=..., name=['area', 'km', '2'], unsplit_name='area km 2', orig_name='Area_km_2', type='number', foreign_key_for=None), Column(id=5, table=..., name=['population'], unsplit_name='population', orig_name='Population', type='number', foreign_key_for=None), Column(id=6, table=..., name=['census', 'ranking'], unsplit_name='census ranking', orig_name='Census_Ranking', type='text', foreign_key_for=None)], primary_keys=[Column(id=1, table=..., name=['city', 'id'], unsplit_name='city id', orig_name='City_ID', type='number', foreign_key_for=None)]), name=['status'], unsplit_name='status', orig_name='Status', type='text', foreign_key_for=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.metadata.schema.columns[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asdas_adsdas']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"asdas_adsdas\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['city'], ['farm'], ['farm', 'competition'], ['competition', 'record']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DB connections: 100%|██████████| 166/166 [00:00<00:00, 354.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from dataset_readers.ratsql_spider_bert import *\n",
    "\n",
    "# builder = RelationBuilder()\n",
    "\n",
    "\n",
    "db_path = \"dataset/database\"\n",
    "pre = SpiderEncoderV2Preproc()\n",
    "schemas, eval_foreign_key_maps = pre.load_tables([\"dataset/tables.json\"])\n",
    "# for path in paths:\n",
    "for db_id, schema in tqdm(schemas.items(), desc=\"DB connections\"):\n",
    "    sqlite_path = Path(db_path) / db_id / f\"{db_id}.sqlite\"\n",
    "    source: sqlite3.Connection\n",
    "    with sqlite3.connect(sqlite_path) as source:\n",
    "        dest = sqlite3.connect(':memory:')\n",
    "        dest.row_factory = sqlite3.Row\n",
    "        source.backup(dest)\n",
    "    schema.connection = dest\n",
    "    \n",
    "examples=[]\n",
    "raw_data = json.load(open(\"dataset/train_spider.json\"))\n",
    "for entry in raw_data:\n",
    "    item = SpiderItem(\n",
    "        text=entry['question_toks'],\n",
    "        code=entry['sql'],\n",
    "        schema=schemas[entry['db_id']],\n",
    "        orig=entry,\n",
    "        orig_schema=schemas[entry['db_id']].orig)\n",
    "#     desc = pre.preprocess_item(item,\"train\")\n",
    "    examples.append(item)\n",
    "\n",
    "        # Backup in-memory copies of all the DBs and create the live connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How', 'many', 'heads', 'of', 'the', 'departments', 'are', 'older', 'than', '56', '?', '<type: text>_*_<table-sep>_<any-table>', '<type: number>_department_id_<table-sep>_department', '<type: text>_name_<table-sep>_department', '<type: text>_creation_<table-sep>_department', '<type: number>_ranking_<table-sep>_department', '<type: number>_budget_in_billions_<table-sep>_department', '<type: number>_num_employees_<table-sep>_department', '<type: number>_head_id_<table-sep>_head', '<type: text>_name_<table-sep>_head', '<type: text>_born_state_<table-sep>_head', '<type: number>_age_<table-sep>_head', '<type: number>_department_id_<table-sep>_management', '<type: number>_head_id_<table-sep>_management', '<type: text>_temporary_acting_<table-sep>_management', 'department', 'head', 'management']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# examples\n",
    "# \n",
    "import pandas as pd\n",
    "print \n",
    "# \n",
    "import tqdm\n",
    "for item in tqdm.tqdm(examples):\n",
    "    desc = pre.preprocess_item(item,\"train\")\n",
    "    q = desc['question']\n",
    "    q_len = len(q)\n",
    "    t = [x[0] for x in  desc['tables']]\n",
    "    t_len = len(t)\n",
    "    c = [\"_\".join(x) for x in  desc['columns']]\n",
    "    c_len = len(c)\n",
    "    enc = q+c+t\n",
    "    print(enc)\n",
    "    relation = pre.compute_relations(desc,len(enc),q_len,c_len,range(c_len+1),range(t_len+1))\n",
    "#     with np.printoptions(threshold=np.inf):\n",
    "#     print(np.array2string())\n",
    "\n",
    "    arr = pd.DataFrame(relation)\n",
    "#     print()\n",
    "    break\n",
    "# arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('utterance', <allennlp.data.fields.text_field.TextField object at 0x7f36567d44c0>)\n",
      "('valid_actions', <allennlp.data.fields.list_field.ListField object at 0x7f3660a35fd0>)\n",
      "('action_sequence', <allennlp.data.fields.list_field.ListField object at 0x7f36565e9730>)\n",
      "('world', <allennlp.data.fields.metadata_field.MetadataField object at 0x7f36565e9d00>)\n",
      "('schema', <dataset_readers.fields.spider_knowledge_graph_field.SpiderKnowledgeGraphField object at 0x7f36565e0be0>)\n"
     ]
    }
   ],
   "source": [
    "for v in a.items():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a.fields['utterance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.sequence_length()\n",
    "b.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=a.fields['schema']\n",
    "c.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ohadr/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.compute_relations(dict(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.to_csv(\"t.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('3.8.0': pyenv)",
   "language": "python",
   "name": "python38064bit380pyenvd6170fe0cd77433db7c61a8427799ead"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
