{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from allennlp.data.token_indexers import PretrainedTransformerMismatchedIndexer\n",
    "# import jsonpickle\n",
    "# a= PretrainedTransformerMismatchedIndexer(\"distilbert-base-uncased\")\n",
    "# a_pickled = jsonpickle.dumps(a)\n",
    "# b = jsonpickle.loads(a_pickled)\n",
    "# b._tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['column:foreign:management:department_id',\n",
       " 'column:foreign:management:head_id',\n",
       " 'column:number:department:budget_in_billions',\n",
       " 'column:number:department:num_employees',\n",
       " 'column:number:department:ranking',\n",
       " 'column:number:head:age',\n",
       " 'column:primary:department:department_id',\n",
       " 'column:primary:head:head_id',\n",
       " 'column:text:department:creation',\n",
       " 'column:text:department:name',\n",
       " 'column:text:head:born_state',\n",
       " 'column:text:head:name',\n",
       " 'column:text:management:temporary_acting',\n",
       " 'table:department',\n",
       " 'table:head',\n",
       " 'table:management']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=['column:foreign:management:department_id', 'column:foreign:management:head_id', 'column:number:department:budget_in_billions', 'column:number:department:num_employees', 'column:number:department:ranking', 'column:number:head:age', 'column:primary:department:department_id', 'column:primary:head:head_id', 'column:text:department:creation', 'column:text:department:name', 'column:text:head:born_state', 'column:text:head:name', 'column:text:management:temporary_acting', 'table:department', 'table:head', 'table:management']\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['column:foreign:management:department_id',\n",
       " 'column:foreign:management:head_id',\n",
       " 'column:number:department:budget_in_billions',\n",
       " 'column:number:department:num_employees',\n",
       " 'column:number:department:ranking',\n",
       " 'column:number:head:age',\n",
       " 'column:primary:department:department_id',\n",
       " 'column:primary:head:head_id',\n",
       " 'column:text:department:creation',\n",
       " 'column:text:department:name',\n",
       " 'column:text:head:born_state',\n",
       " 'column:text:head:name',\n",
       " 'column:text:management:temporary_acting',\n",
       " 'table:department',\n",
       " 'table:head',\n",
       " 'table:management']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp_to_key(mycmp):\n",
    "    'Convert a cmp= function into a key= function'\n",
    "    class K:\n",
    "        def __init__(self, obj, *args):\n",
    "            self.obj = obj\n",
    "        def __lt__(self, other):\n",
    "            return mycmp(self.obj, other.obj) < 0\n",
    "        def __gt__(self, other):\n",
    "            return mycmp(self.obj, other.obj) > 0\n",
    "        def __eq__(self, other):\n",
    "            return mycmp(self.obj, other.obj) == 0\n",
    "        def __le__(self, other):\n",
    "            return mycmp(self.obj, other.obj) <= 0\n",
    "        def __ge__(self, other):\n",
    "            return mycmp(self.obj, other.obj) >= 0\n",
    "        def __ne__(self, other):\n",
    "            return mycmp(self.obj, other.obj) != 0\n",
    "    return K\n",
    "\n",
    "def cmp_f(x,y):\n",
    "    _x = x.split(\":\")\n",
    "    _y = y.split(\":\")\n",
    "    if len(_x)==2:\n",
    "        _x = _x[1]\n",
    "    else:\n",
    "        _x = _x[2]+\":\"+_x[3]\n",
    "    if len(_y)==2:\n",
    "        _y = _y[1]\n",
    "    else:\n",
    "        _y = _y[2]+\":\"+_y[3]\n",
    "#     c = \n",
    "#     print(c)\n",
    "    return (_x > _y) - (_x < _y)\n",
    "#     return int(_x < _y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['table:department',\n",
       " 'column:number:department:budget_in_billions',\n",
       " 'column:text:department:creation',\n",
       " 'column:primary:department:department_id',\n",
       " 'column:text:department:name',\n",
       " 'column:number:department:num_employees',\n",
       " 'column:number:department:ranking',\n",
       " 'table:head',\n",
       " 'column:number:head:age',\n",
       " 'column:text:head:born_state',\n",
       " 'column:primary:head:head_id',\n",
       " 'column:text:head:name',\n",
       " 'table:management',\n",
       " 'column:foreign:management:department_id',\n",
       " 'column:foreign:management:head_id',\n",
       " 'column:text:management:temporary_acting']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=sorted(a,key=cmp_to_key(cmp_f))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department(budget_in_billions:number,creation:text,department_id:primary,name:text,num_employees:number,ranking:number)\n",
      "head(age:number,born_state:text,head_id:primary,name:text)\n",
      "management(department_id:foreign,head_id:foreign,temporary_acting:text)\n",
      "\n",
      "['department', '(', 'budget_in_billions:number', ',', 'creation:text', ',', 'department_id:primary', ',', 'name:text', ',', 'num_employees:number', ',', 'ranking:number', ')\\n', 'head', '(', 'age:number', ',', 'born_state:text', ',', 'head_id:primary', ',', 'name:text', ')\\n', 'management', '(', 'department_id:foreign', ',', 'head_id:foreign', ',', 'temporary_acting:text', ')\\n']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_peren(b):\n",
    "    c = []\n",
    "    d = [\":\".join(x.split(\":\")[1:]) for x in b]\n",
    "    append_list = []\n",
    "    for i,x in enumerate(d):\n",
    "        curr = x\n",
    "        if \":\" in curr: #col\n",
    "            col_type,_,col_name = curr.split(\":\")\n",
    "            curr = f\"{col_name}:{col_type}\"\n",
    "            c.append(curr)\n",
    "            if (i+1)<len(d) and \":\" in d[i+1]:\n",
    "                c.append(\",\")\n",
    "            else:\n",
    "                c.append(\")\\n\")\n",
    "            append_list.extend([True,False])\n",
    "\n",
    "        else:\n",
    "    #         c.append(curr)\n",
    "\n",
    "            c.append(curr)\n",
    "            c.append(\"(\")\n",
    "            append_list.extend([True,False])\n",
    "    return c,append_list\n",
    "c,h = add_peren(b)\n",
    "print(\"\".join(c))\n",
    "print(c)\n",
    "h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cmp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c1adf8efc435>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'cmp' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-7376b7cbd38e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m\"ab\"\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "\"ab\" -\"c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(10000000000000)):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c983ab47eb3047759649cf9a191f6c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', max=1.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b267736827477fbe2e958ee9937a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='building vocab', max=646.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[“CUDA_DEVICE_ORDER”]=“PCI_BUS_ID”\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# experiment_name = \"3_heads_lr3_keep_op_identity+agenda_enriched_all+lr_e3+mult_scalar_per_action+glove\"\n",
    "# experiment_name = \"crappy-red-dhole\"\n",
    "# train_dataset = reader.read(\"dataset/train_spider.json\")\n",
    "from models.semantic_parsing.ratsql_encoder import RatsqlEncoder\n",
    "# from models.semantic_parsing.gnn_encoder import GnnEncoder\n",
    "from models.semantic_parsing.spider_decoder import SpiderParser\n",
    "from allennlp.modules.seq2vec_encoders.boe_encoder import BagOfEmbeddingsEncoder\n",
    "\n",
    "from allennlp.modules.attention import DotProductAttention\n",
    "from allennlp.nn.beam_search import BeamSearch\n",
    "from allennlp.modules.seq2seq_encoders.pass_through_encoder import PassThroughEncoder\n",
    "\n",
    "\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "\n",
    "import torch.optim as optim\n",
    "from allennlp.training.trainer import Trainer\n",
    "import torch\n",
    "from allennlp.models.archival import Archive\n",
    "import torch\n",
    "from allennlp.common import Params\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.common.params import with_fallback\n",
    "from dataset_readers.spider_ratsql import SpiderRatsqlDatasetReader\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "\n",
    "# from models.semantic_parsing.spider_parser import SpiderParser\n",
    "# reader = SpiderRatsqlDatasetReader(tables_file=\"dataset/tables.json\",max_instances=None)\n",
    "reader = SpiderRatsqlDatasetReader(tables_file=\"dataset/tables.json\",max_instances=1000)\n",
    "# settings = Params.from_file(f\"experiments/{experiment_name}/config.json\")\n",
    "# model = Model.load(config=settings, serialization_dir=f\"experiments/{experiment_name}\")\n",
    "\n",
    "\n",
    "train_dataset = reader.read(\"dataset/train_spider.json\")\n",
    "vocab = Vocabulary.from_instances(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with torch.cuda.device(0):\n",
    "EMBEDDING_DIM = 768\n",
    "HIDDEN_DIM = 768\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "\n",
    "beam = BeamSearch(end_index=0,beam_size=10)\n",
    "\n",
    "schema_encoder = RatsqlEncoder(encoder=PassThroughEncoder(768),entity_encoder=BagOfEmbeddingsEncoder(768),question_embedder=word_embeddings,action_embedding_dim=768)\n",
    "# schema_encoder = GnnEncoder(encoder=PassThroughEncoder(200),entity_encoder=BagOfEmbeddingsEncoder(200),question_embedder=word_embeddings,action_embedding_dim=200)\n",
    "model = SpiderParser(vocab=vocab,schema_encoder=schema_encoder, \n",
    "                     decoder_beam_search=beam,input_attention=DotProductAttention(),past_attention=DotProductAttention(),max_decoding_steps=10)\n",
    "\n",
    "model.cuda()\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=None\n",
    "for inst in train_dataset:\n",
    "    a = inst\n",
    "    res_list = model.forward_on_instances([inst])\n",
    "    print(res_list)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = BasicIterator(batch_size=15)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# iterator.index_with(vocab)\n",
    "# trainer = Trainer(model=model,\n",
    "#                   optimizer=optimizer,\n",
    "#                   iterator=iterator,\n",
    "#                   train_dataset=train_dataset,\n",
    "#                   validation_dataset=train_dataset,\n",
    "#                   patience=10,\n",
    "#                   num_epochs=1)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.fields    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import  allennlp.nn.util as util\n",
    "import torch\n",
    "\n",
    "def batched_span_select(target: torch.Tensor, spans: torch.LongTensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    The given `spans` of size `(batch_size, num_spans, 2)` indexes into the sequence\n",
    "    dimension (dimension 2) of the target, which has size `(batch_size, sequence_length,\n",
    "    embedding_size)`.\n",
    "    This function returns segmented spans in the target with respect to the provided span indices.\n",
    "    It does not guarantee element order within each span.\n",
    "    # Parameters\n",
    "    target : `torch.Tensor`, required.\n",
    "        A 3 dimensional tensor of shape (batch_size, sequence_length, embedding_size).\n",
    "        This is the tensor to be indexed.\n",
    "    indices : `torch.LongTensor`\n",
    "        A 3 dimensional tensor of shape (batch_size, num_spans, 2) representing start and end\n",
    "        indices (both inclusive) into the `sequence_length` dimension of the `target` tensor.\n",
    "    # Returns\n",
    "    span_embeddings : `torch.Tensor`\n",
    "        A tensor with shape (batch_size, num_spans, max_batch_span_width, embedding_size]\n",
    "        representing the embedded spans extracted from the batch flattened target tensor.\n",
    "    span_mask: `torch.BoolTensor`\n",
    "        A tensor with shape (batch_size, num_spans, max_batch_span_width) representing the mask on\n",
    "        the returned span embeddings.\n",
    "    \"\"\"\n",
    "    # both of shape (batch_size, num_spans, 1)\n",
    "    span_starts, span_ends = spans.split(1, dim=-1)\n",
    "\n",
    "    # shape (batch_size, num_spans, 1)\n",
    "    # These span widths are off by 1, because the span ends are `inclusive`.\n",
    "    span_widths = span_ends - span_starts\n",
    "\n",
    "    # We need to know the maximum span width so we can\n",
    "    # generate indices to extract the spans from the sequence tensor.\n",
    "    # These indices will then get masked below, such that if the length\n",
    "    # of a given span is smaller than the max, the rest of the values\n",
    "    # are masked.\n",
    "    max_batch_span_width = span_widths.max().item() + 1\n",
    "\n",
    "    # Shape: (1, 1, max_batch_span_width)\n",
    "    max_span_range_indices = util.get_range_vector(max_batch_span_width, util.get_device_of(target)).view(\n",
    "        1, 1, -1\n",
    "    )\n",
    "#     print(max_batch_span_width)\n",
    "#     print(max_span_range_indices)\n",
    "    # Shape: (batch_size, num_spans, max_batch_span_width)\n",
    "    # This is a broadcasted comparison - for each span we are considering,\n",
    "    # we are creating a range vector of size max_span_width, but masking values\n",
    "    # which are greater than the actual length of the span.\n",
    "    #\n",
    "    # We're using <= here (and for the mask below) because the span ends are\n",
    "    # inclusive, so we want to include indices which are equal to span_widths rather\n",
    "    # than using it as a non-inclusive upper bound.\n",
    "    span_mask = max_span_range_indices <= span_widths\n",
    "#     raw_span_indices = span_ends - max_span_range_indices\n",
    "    raw_span_indices = span_starts + max_span_range_indices\n",
    "#     print(raw_span_indices)\n",
    "#     print(target.size())\n",
    "    # We also don't want to include span indices which are less than zero,\n",
    "    # which happens because some spans near the beginning of the sequence\n",
    "    # have an end index < max_batch_span_width, so we add this to the mask here.\n",
    "    span_mask = span_mask & (raw_span_indices < target.size(1))\n",
    "#     print(span_mask)\n",
    "#     span_indices = torch.nn.functional.relu(raw_span_indices.float()).long()\n",
    "    span_indices = raw_span_indices * span_mask\n",
    "#     print(span_indices)\n",
    "    \n",
    "    # Shape: (batch_size, num_spans, max_batch_span_width, embedding_dim)\n",
    "    span_embeddings = util.batched_index_select(target, span_indices)\n",
    "\n",
    "    return span_embeddings, span_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch.tensor\n",
    "a = util.get_range_vector(45,-1).reshape([3,15,1])\n",
    "print(a)\n",
    "q = 2\n",
    "t = 3\n",
    "b = torch.tensor([[0,q-1],[q,q+t-1]])\n",
    "# b = torch.tensor([[[1,3]],[[2,3]],[[3,4]]])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_span_select(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.batched_span_select(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list()\n",
    "import numpy as np\n",
    "b = a.fields['enc']\n",
    "token_ids = [0]+b.tokens\n",
    "c = a.fields['lengths'].array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'column_names': [[-1, '*'], [0, 'perpetrator id'], [0, 'people id'], [0, 'date'], [0, 'year'], [0, 'location'], [0, 'country'], [0, 'killed'], [0, 'injured'], [1, 'people id'], [1, 'name'], [1, 'height'], [1, 'weight'], [1, 'home town']], 'column_names_original': [[-1, '*'], [0, 'Perpetrator_ID'], [0, 'People_ID'], [0, 'Date'], [0, 'Year'], [0, 'Location'], [0, 'Country'], [0, 'Killed'], [0, 'Injured'], [1, 'People_ID'], [1, 'Name'], [1, 'Height'], [1, 'Weight'], [1, 'Home Town']], 'column_types': ['text', 'number', 'number', 'text', 'number', 'text', 'text', 'number', 'number', 'number', 'text', 'number', 'number', 'text'], 'db_id': 'perpetrator', 'foreign_keys': [[2, 9]], 'primary_keys': [1, 9], 'table_names': ['perpetrator', 'people'], 'table_names_original': ['perpetrator', 'people']}\n",
      "[[-1, '*'], [0, 'Perpetrator_ID'], [0, 'People_ID'], [0, 'Date'], [0, 'Year'], [0, 'Location'], [0, 'Country'], [0, 'Killed'], [0, 'Injured'], [1, 'People_ID'], [1, 'Name'], [1, 'Height'], [1, 'Weight'], [1, 'Home Town']]\n",
      "[[-1, '*'], [0, 'perpetrator id'], [0, 'people id'], [0, 'date'], [0, 'year'], [0, 'location'], [0, 'country'], [0, 'killed'], [0, 'injured'], [1, 'people id'], [1, 'name'], [1, 'height'], [1, 'weight'], [1, 'home town']]\n",
      "{'column_names': [[-1, '*'], [0, 'building'], [0, 'room number'], [0, 'capacity'], [1, 'department name'], [1, 'building'], [1, 'budget'], [2, 'course id'], [2, 'title'], [2, 'department name'], [2, 'credits'], [3, 'id'], [3, 'name'], [3, 'department name'], [3, 'salary'], [4, 'course id'], [4, 'section id'], [4, 'semester'], [4, 'year'], [4, 'building'], [4, 'room number'], [4, 'time slot id'], [5, 'id'], [5, 'course id'], [5, 'section id'], [5, 'semester'], [5, 'year'], [6, 'id'], [6, 'name'], [6, 'department name'], [6, 'total credits'], [7, 'id'], [7, 'course id'], [7, 'section id'], [7, 'semester'], [7, 'year'], [7, 'grade'], [8, 'student id'], [8, 'instructor id'], [9, 'time slot id'], [9, 'day'], [9, 'start hour'], [9, 'start minute'], [9, 'end hour'], [9, 'end minute'], [10, 'course id'], [10, 'prerequisite id']], 'column_names_original': [[-1, '*'], [0, 'building'], [0, 'room_number'], [0, 'capacity'], [1, 'dept_name'], [1, 'building'], [1, 'budget'], [2, 'course_id'], [2, 'title'], [2, 'dept_name'], [2, 'credits'], [3, 'ID'], [3, 'name'], [3, 'dept_name'], [3, 'salary'], [4, 'course_id'], [4, 'sec_id'], [4, 'semester'], [4, 'year'], [4, 'building'], [4, 'room_number'], [4, 'time_slot_id'], [5, 'ID'], [5, 'course_id'], [5, 'sec_id'], [5, 'semester'], [5, 'year'], [6, 'ID'], [6, 'name'], [6, 'dept_name'], [6, 'tot_cred'], [7, 'ID'], [7, 'course_id'], [7, 'sec_id'], [7, 'semester'], [7, 'year'], [7, 'grade'], [8, 's_ID'], [8, 'i_ID'], [9, 'time_slot_id'], [9, 'day'], [9, 'start_hr'], [9, 'start_min'], [9, 'end_hr'], [9, 'end_min'], [10, 'course_id'], [10, 'prereq_id']], 'column_types': ['text', 'text', 'text', 'number', 'text', 'text', 'number', 'text', 'text', 'text', 'number', 'text', 'text', 'text', 'number', 'text', 'text', 'text', 'number', 'text', 'text', 'text', 'text', 'text', 'text', 'text', 'number', 'text', 'text', 'text', 'number', 'text', 'text', 'text', 'text', 'number', 'text', 'text', 'text', 'text', 'text', 'number', 'number', 'number', 'number', 'text', 'text'], 'db_id': 'college_2', 'foreign_keys': [[9, 4], [13, 4], [19, 1], [20, 2], [15, 7], [22, 11], [23, 15], [24, 16], [25, 17], [26, 18], [29, 4], [31, 27], [32, 15], [33, 16], [34, 17], [35, 18], [37, 27], [38, 11], [46, 7], [45, 7]], 'primary_keys': [1, 4, 7, 11, 15, 22, 27, 31, 37, 39, 45], 'table_names': ['classroom', 'department', 'course', 'instructor', 'section', 'teaches', 'student', 'takes classes', 'advisor', 'time slot', 'prerequisite'], 'table_names_original': ['classroom', 'department', 'course', 'instructor', 'section', 'teaches', 'student', 'takes', 'advisor', 'time_slot', 'prereq']}\n",
      "[[-1, '*'], [0, 'building'], [0, 'room_number'], [0, 'capacity'], [1, 'dept_name'], [1, 'building'], [1, 'budget'], [2, 'course_id'], [2, 'title'], [2, 'dept_name'], [2, 'credits'], [3, 'ID'], [3, 'name'], [3, 'dept_name'], [3, 'salary'], [4, 'course_id'], [4, 'sec_id'], [4, 'semester'], [4, 'year'], [4, 'building'], [4, 'room_number'], [4, 'time_slot_id'], [5, 'ID'], [5, 'course_id'], [5, 'sec_id'], [5, 'semester'], [5, 'year'], [6, 'ID'], [6, 'name'], [6, 'dept_name'], [6, 'tot_cred'], [7, 'ID'], [7, 'course_id'], [7, 'sec_id'], [7, 'semester'], [7, 'year'], [7, 'grade'], [8, 's_ID'], [8, 'i_ID'], [9, 'time_slot_id'], [9, 'day'], [9, 'start_hr'], [9, 'start_min'], [9, 'end_hr'], [9, 'end_min'], [10, 'course_id'], [10, 'prereq_id']]\n",
      "[[-1, '*'], [0, 'building'], [0, 'room number'], [0, 'capacity'], [1, 'department name'], [1, 'building'], [1, 'budget'], [2, 'course id'], [2, 'title'], [2, 'department name'], [2, 'credits'], [3, 'id'], [3, 'name'], [3, 'department name'], [3, 'salary'], [4, 'course id'], [4, 'section id'], [4, 'semester'], [4, 'year'], [4, 'building'], [4, 'room number'], [4, 'time slot id'], [5, 'id'], [5, 'course id'], [5, 'section id'], [5, 'semester'], [5, 'year'], [6, 'id'], [6, 'name'], [6, 'department name'], [6, 'total credits'], [7, 'id'], [7, 'course id'], [7, 'section id'], [7, 'semester'], [7, 'year'], [7, 'grade'], [8, 'student id'], [8, 'instructor id'], [9, 'time slot id'], [9, 'day'], [9, 'start hour'], [9, 'start minute'], [9, 'end hour'], [9, 'end minute'], [10, 'course id'], [10, 'prerequisite id']]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "schema_path = \"dataset/tables.json\"\n",
    "with open(schema_path, \"r\") as f:\n",
    "    dbs_json_blob = json.load(f)\n",
    "    for i,db in enumerate(dbs_json_blob):\n",
    "        print(db)\n",
    "        print(db['column_names_original'])\n",
    "#         diff = set(x.lower().re )\n",
    "        print(db['column_names'])\n",
    "        if i == 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0]+token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = b.tokens[0]\n",
    "# vars(y)\n",
    "y.text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(c.array)\n",
    "offsets = get_offsets(c)\n",
    "res = [token_ids[offsets[j][0]:offsets[j][1] + 1] for j in range(len(offsets))]\n",
    "# print(res)\n",
    "# print()\n",
    "# for x in [[vocab.get_token_from_index(y,\"tags\") for y in x] for x in res]:\n",
    "for x in res:\n",
    "    print([y.text_id for y in x])\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([1, 205, 2])\n",
    "torch.Size([1, 205, 768])\n",
    "tensor([[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 16, 13, 12, 12, 12, 14, 14,\n",
    "         13, 12, 13, 12, 13, 13, 13,  4,  4,  4]], dtype=torch.int32) torch.Size([1, 28])\n",
    "tensor([2129])\n",
    "tensor([2116])\n",
    "tensor([4641])\n",
    "tensor([1997])\n",
    "tensor([1996])\n",
    "tensor([7640])\n",
    "tensor([2024])\n",
    "tensor([3080])\n",
    "tensor([2084])\n",
    "tensor([5179])\n",
    "tensor([1029])\n",
    "tensor([ 1026,  2828,  1024,  3793,  1028,  1008,  1026,  2795,  1011, 19802,\n",
    "         1028,  1026,  2151,  1011,  2795,  1028])\n",
    "tensor([ 1026,  2828,  1024,  2193,  1028,  2533,  8909,  1026,  2795,  1011,\n",
    "        19802,  1028,  2533])\n",
    "tensor([ 1026,  2828,  1024,  3793,  1028,  2171,  1026,  2795,  1011, 19802,\n",
    "         1028,  2533])\n",
    "tensor([ 1026,  2828,  1024,  3793,  1028,  4325,  1026,  2795,  1011, 19802,\n",
    "         1028,  2533])\n",
    "tensor([ 1026,  2828,  1024,  2193,  1028,  5464,  1026,  2795,  1011, 19802,\n",
    "         1028,  2533])\n",
    "tensor([ 1026,  2828,  1024,  2193,  1028,  5166,  1999, 25501,  1026,  2795,\n",
    "         1011, 19802,  1028,  2533])\n",
    "tensor([ 1026,  2828,  1024,  2193,  1028, 16371,  1001,  1001,  1049,  5126,\n",
    "         1026,  2795,  1011, 19802])\n",
    "tensor([ 1028,  2533,  1026,  2828,  1024,  2193,  1028,  2132,  8909,  1026,\n",
    "         2795,  1011, 19802])\n",
    "tensor([ 1028,  2132,  1026,  2828,  1024,  3793,  1028,  2171,  1026,  2795,\n",
    "         1011, 19802])\n",
    "tensor([ 1028,  2132,  1026,  2828,  1024,  3793,  1028,  2141,  2110,  1026,\n",
    "         2795,  1011, 19802])\n",
    "tensor([ 1028,  2132,  1026,  2828,  1024,  2193,  1028,  2287,  1026,  2795,\n",
    "         1011, 19802])\n",
    "tensor([ 1028,  2132,  1026,  2828,  1024,  2193,  1028,  2533,  8909,  1026,\n",
    "         2795,  1011, 19802])\n",
    "tensor([ 1028,  2968,  1026,  2828,  1024,  2193,  1028,  2132,  8909,  1026,\n",
    "         2795,  1011, 19802])\n",
    "tensor([ 1028,  2968,  1026,  2828,  1024,  3793,  1028,  5741,  3772,  1026,\n",
    "         2795,  1011, 19802])\n",
    "tensor([1028, 2968, 1026, 2795])\n",
    "tensor([1028, 2533, 1026, 2795])\n",
    "tensor([1028, 2132, 1026, 2795])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=None\n",
    "for inst in train_dataset:\n",
    "    a = inst\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset)\n",
    "b=a.fields['enc']\n",
    "b.index(vocab)\n",
    "tok_dict = b._indexed_tokens['tokens']\n",
    "print(len(tok_dict['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "c=a.fields['lengths'].array\n",
    "token_ids = tok_dict['token_ids']\n",
    "old_offsets = tok_dict['offsets']\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_str = lambda x: [vocab.get_token_from_index(y,\"tags\") for y in x]\n",
    "lengths = np.array(list(c))+1\n",
    "# offsets = np.cumsum(lengths-1)\n",
    "# offsets\n",
    "# print(lengths)\n",
    "\n",
    "# offsets = list(zip(offsets,offsets)\n",
    "# offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_ = \n",
    "# b_=offsets+lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_offsets(lengths):\n",
    "    e = np.cumsum(([0]+list(lengths))[:-1])\n",
    "    return list(zip(e+1,e+np.array(lengths)))\n",
    "\n",
    "offsets = get_offsets(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _token_indexers\n",
    "# to_str = lambda x: vocab.get_token_from_index(x,\"tags\")\n",
    "# print(c.array)\n",
    "res = [token_ids[offsets[j][0]:offsets[j][1] + 1] for j in range(len(offsets))]\n",
    "# print(res)\n",
    "# print()\n",
    "for x in [[vocab.get_token_from_index(y,\"tags\") for y in x] for x in res]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.get_token_from_index(102,'tags')\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= b._token_indexers['tokens']\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.indices_to_tokens([23],vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward_on_instances([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "i=random.randint(0,len(train_dataset))\n",
    "a=None\n",
    "for inst in train_dataset[i:i+1]:\n",
    "    a = inst\n",
    "    res_list = model.forward_on_instances([inst])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.fields['desc'].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= res_list[0]['initial_state']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= b.get_valid_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.get_valid_actions()\n",
    "b.possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "i=random.randint(0,len(train_dataset))\n",
    "a=None\n",
    "for inst in train_dataset[i:i+1]:\n",
    "#     print(inst)\n",
    "    a=inst\n",
    "# b=a.fields['relation']\n",
    "# b.array\n",
    "b=a.fields['world']\n",
    "c=a.fields['desc']\n",
    "d=a.fields['item'] \n",
    "e = a.fields[\"schema\"]\n",
    "# b\n",
    "# print(a.fields['utterance'])\n",
    "for entity in b.metadata.db_context.knowledge_graph.entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in (c.metadata['columns']+c.metadata['tables']):\n",
    "def normalize_schema_constant(entity):\n",
    "#     print(entity)\n",
    "    col = \"_\".join(entity)\n",
    "    col =  col.split(\"_<table-sep>_\")\n",
    "    if len(col)==1:\n",
    "        return \"_\".join(entity)\n",
    "#         print()\n",
    "    else:\n",
    "        table = col[1]\n",
    "        col = col[0].split(\">_\")[1]\n",
    "        return f\"{table}@{col}\"\n",
    "#         print()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.entity_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.fields['valid_actions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.metadata.schema.columns[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"asdas_adsdas\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataset_readers.ratsql_spider_bert import *\n",
    "\n",
    "# builder = RelationBuilder()\n",
    "\n",
    "\n",
    "db_path = \"dataset/database\"\n",
    "pre = SpiderEncoderV2Preproc()\n",
    "schemas, eval_foreign_key_maps = pre.load_tables([\"dataset/tables.json\"])\n",
    "# for path in paths:\n",
    "for db_id, schema in tqdm(schemas.items(), desc=\"DB connections\"):\n",
    "    sqlite_path = Path(db_path) / db_id / f\"{db_id}.sqlite\"\n",
    "    source: sqlite3.Connection\n",
    "    with sqlite3.connect(sqlite_path) as source:\n",
    "        dest = sqlite3.connect(':memory:')\n",
    "        dest.row_factory = sqlite3.Row\n",
    "        source.backup(dest)\n",
    "    schema.connection = dest\n",
    "    \n",
    "examples=[]\n",
    "raw_data = json.load(open(\"dataset/train_spider.json\"))\n",
    "for entry in raw_data:\n",
    "    item = SpiderItem(\n",
    "        text=entry['question_toks'],\n",
    "        code=entry['sql'],\n",
    "        schema=schemas[entry['db_id']],\n",
    "        orig=entry,\n",
    "        orig_schema=schemas[entry['db_id']].orig)\n",
    "#     desc = pre.preprocess_item(item,\"train\")\n",
    "    examples.append(item)\n",
    "\n",
    "        # Backup in-memory copies of all the DBs and create the live connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples\n",
    "# \n",
    "import pandas as pd\n",
    "print \n",
    "# \n",
    "import tqdm\n",
    "for item in tqdm.tqdm(examples):\n",
    "    desc = pre.preprocess_item(item,\"train\")\n",
    "    q = desc['question']\n",
    "    q_len = len(q)\n",
    "    t = [x[0] for x in  desc['tables']]\n",
    "    t_len = len(t)\n",
    "    c = [\"_\".join(x) for x in  desc['columns']]\n",
    "    c_len = len(c)\n",
    "    enc = q+c+t\n",
    "    print(enc)\n",
    "    relation = pre.compute_relations(desc,len(enc),q_len,c_len,range(c_len+1),range(t_len+1))\n",
    "#     with np.printoptions(threshold=np.inf):\n",
    "#     print(np.array2string())\n",
    "\n",
    "    arr = pd.DataFrame(relation)\n",
    "#     print()\n",
    "    break\n",
    "# arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in a.items():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a.fields['utterance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.sequence_length()\n",
    "b.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=a.fields['schema']\n",
    "c.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.compute_relations(dict(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.to_csv(\"t.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
