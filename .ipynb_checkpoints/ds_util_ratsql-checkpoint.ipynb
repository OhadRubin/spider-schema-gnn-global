{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from allennlp.data.token_indexers import PretrainedTransformerMismatchedIndexer\n",
    "# import jsonpickle\n",
    "# a= PretrainedTransformerMismatchedIndexer(\"distilbert-base-uncased\")\n",
    "# a_pickled = jsonpickle.dumps(a)\n",
    "# b = jsonpickle.loads(a_pickled)\n",
    "# b._tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 38243613/10000000000000 [00:07<533:34:28, 5205957.98it/s]"
     ]
    }
   ],
   "source": [
    "# for i in tqdm(range(10000000000000)):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57edaa1cb5a845f7889b59280fd49cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', max=1.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8c8c919cc923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset/train_spider.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/specific_ohadr/netapp5/joberant/home/ohadr/.conda/cap/lib/python3.7/site-packages/allennlp/data/dataset_readers/dataset_reader.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;31m# Then some validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                 \u001b[0minstances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/specific_ohadr/netapp5/joberant/home/ohadr/.conda/cap/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/specific_ohadr/netapp5/joberant/home/ohadr/.conda/cap/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2020A/spider-schema-gnn-global/dataset_readers/spider_ratsql.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_examples_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2020A/spider-schema-gnn-global/dataset_readers/spider_ratsql.py\u001b[0m in \u001b[0;36m_read_examples_file\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 ins = self.text_to_instance(\n\u001b[1;32m    285\u001b[0m                     \u001b[0mutterance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mdb_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'db_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     sql=query_tokens,orig=ex)\n\u001b[1;32m    288\u001b[0m                 \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_cnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2020A/spider-schema-gnn-global/dataset_readers/spider_ratsql.py\u001b[0m in \u001b[0;36mtext_to_instance\u001b[0;34m(self, utterance, db_id, sql, orig)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0morig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             orig_schema=self.schemas[db_id].orig)\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0;31m# fields[\"item\"] = MetadataField(item)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;31m# fields[\"desc\"] = MetadataField(desc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2020A/spider-schema-gnn-global/dataset_readers/spider_ratsql.py\u001b[0m in \u001b[0;36mpreprocess_item\u001b[0;34m(self, item, validation_info)\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mcolumn_names_without_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreproc_schema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0msc_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_schema_linking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_names_without_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreproc_schema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0mcv_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_cell_value_linking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2020A/spider-schema-gnn-global/dataset_readers/spider_ratsql.py\u001b[0m in \u001b[0;36mcompute_cell_value_linking\u001b[0;34m(cls, tokens, schema)\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"number\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TODO fine-grained date\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m                         \u001b[0mnum_date_match\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"{q_id},{col_id}\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m                     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb_word_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/2020A/spider-schema-gnn-global/dataset_readers/spider_ratsql.py\u001b[0m in \u001b[0;36mdb_word_match\u001b[0;34m(word, column, table, db_conn)\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0mp_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"select {column} from {table} where {column} like '{word} %' or {column} like '% {word}' or \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m                     \u001b[0;34mf\"{column} like '% {word} %' or {column} like '{word}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m                 \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m                 \u001b[0mp_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[“CUDA_DEVICE_ORDER”]=“PCI_BUS_ID”\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "# experiment_name = \"3_heads_lr3_keep_op_identity+agenda_enriched_all+lr_e3+mult_scalar_per_action+glove\"\n",
    "# experiment_name = \"crappy-red-dhole\"\n",
    "# train_dataset = reader.read(\"dataset/train_spider.json\")\n",
    "from models.semantic_parsing.ratsql_encoder import RatsqlEncoder\n",
    "# from models.semantic_parsing.gnn_encoder import GnnEncoder\n",
    "from models.semantic_parsing.spider_decoder import SpiderParser\n",
    "from allennlp.modules.seq2vec_encoders.boe_encoder import BagOfEmbeddingsEncoder\n",
    "\n",
    "from allennlp.modules.attention import DotProductAttention\n",
    "from allennlp.nn.beam_search import BeamSearch\n",
    "from allennlp.modules.seq2seq_encoders.pass_through_encoder import PassThroughEncoder\n",
    "\n",
    "\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "\n",
    "import torch.optim as optim\n",
    "from allennlp.training.trainer import Trainer\n",
    "import torch\n",
    "from allennlp.models.archival import Archive\n",
    "import torch\n",
    "from allennlp.common import Params\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.common.params import with_fallback\n",
    "from dataset_readers.spider_ratsql import SpiderRatsqlDatasetReader\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "\n",
    "# from models.semantic_parsing.spider_parser import SpiderParser\n",
    "# reader = SpiderRatsqlDatasetReader(tables_file=\"dataset/tables.json\",max_instances=None)\n",
    "reader = SpiderRatsqlDatasetReader(tables_file=\"dataset/tables.json\",max_instances=1000)\n",
    "# settings = Params.from_file(f\"experiments/{experiment_name}/config.json\")\n",
    "# model = Model.load(config=settings, serialization_dir=f\"experiments/{experiment_name}\")\n",
    "\n",
    "\n",
    "train_dataset = reader.read(\"dataset/train_spider.json\")\n",
    "vocab = Vocabulary.from_instances(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.cuda.device(0):\n",
    "EMBEDDING_DIM = 768\n",
    "HIDDEN_DIM = 768\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "\n",
    "beam = BeamSearch(end_index=0,beam_size=10)\n",
    "\n",
    "schema_encoder = RatsqlEncoder(encoder=PassThroughEncoder(768),entity_encoder=BagOfEmbeddingsEncoder(768),question_embedder=word_embeddings,action_embedding_dim=768)\n",
    "# schema_encoder = GnnEncoder(encoder=PassThroughEncoder(200),entity_encoder=BagOfEmbeddingsEncoder(200),question_embedder=word_embeddings,action_embedding_dim=200)\n",
    "model = SpiderParser(vocab=vocab,schema_encoder=schema_encoder, \n",
    "                     decoder_beam_search=beam,input_attention=DotProductAttention(),past_attention=DotProductAttention(),max_decoding_steps=10)\n",
    "\n",
    "model.cuda()\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=None\n",
    "for inst in train_dataset:\n",
    "    a = inst\n",
    "    res_list = model.forward_on_instances([inst])\n",
    "    print(res_list)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "646"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator = BasicIterator(batch_size=15)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# iterator.index_with(vocab)\n",
    "# trainer = Trainer(model=model,\n",
    "#                   optimizer=optimizer,\n",
    "#                   iterator=iterator,\n",
    "#                   train_dataset=train_dataset,\n",
    "#                   validation_dataset=train_dataset,\n",
    "#                   patience=10,\n",
    "#                   num_epochs=1)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'schema_strings': <allennlp.data.fields.metadata_field.MetadataField at 0x7fe751a8acd0>,\n",
       " 'enc': <allennlp.data.fields.text_field.TextField at 0x7fe7531985a0>,\n",
       " 'lengths': <allennlp.data.fields.array_field.ArrayField at 0x7fe751795b90>,\n",
       " 'offsets': <allennlp.data.fields.array_field.ArrayField at 0x7fe751795a00>,\n",
       " 'relation': <allennlp.data.fields.array_field.ArrayField at 0x7fe750a81a50>,\n",
       " 'valid_actions': <allennlp.data.fields.list_field.ListField at 0x7fe750aa6810>,\n",
       " 'action_sequence': <allennlp.data.fields.list_field.ListField at 0x7fe7509efed0>,\n",
       " 'world': <allennlp.data.fields.metadata_field.MetadataField at 0x7fe7509df490>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.fields    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import  allennlp.nn.util as util\n",
    "import torch\n",
    "\n",
    "def batched_span_select(target: torch.Tensor, spans: torch.LongTensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    The given `spans` of size `(batch_size, num_spans, 2)` indexes into the sequence\n",
    "    dimension (dimension 2) of the target, which has size `(batch_size, sequence_length,\n",
    "    embedding_size)`.\n",
    "    This function returns segmented spans in the target with respect to the provided span indices.\n",
    "    It does not guarantee element order within each span.\n",
    "    # Parameters\n",
    "    target : `torch.Tensor`, required.\n",
    "        A 3 dimensional tensor of shape (batch_size, sequence_length, embedding_size).\n",
    "        This is the tensor to be indexed.\n",
    "    indices : `torch.LongTensor`\n",
    "        A 3 dimensional tensor of shape (batch_size, num_spans, 2) representing start and end\n",
    "        indices (both inclusive) into the `sequence_length` dimension of the `target` tensor.\n",
    "    # Returns\n",
    "    span_embeddings : `torch.Tensor`\n",
    "        A tensor with shape (batch_size, num_spans, max_batch_span_width, embedding_size]\n",
    "        representing the embedded spans extracted from the batch flattened target tensor.\n",
    "    span_mask: `torch.BoolTensor`\n",
    "        A tensor with shape (batch_size, num_spans, max_batch_span_width) representing the mask on\n",
    "        the returned span embeddings.\n",
    "    \"\"\"\n",
    "    # both of shape (batch_size, num_spans, 1)\n",
    "    span_starts, span_ends = spans.split(1, dim=-1)\n",
    "\n",
    "    # shape (batch_size, num_spans, 1)\n",
    "    # These span widths are off by 1, because the span ends are `inclusive`.\n",
    "    span_widths = span_ends - span_starts\n",
    "\n",
    "    # We need to know the maximum span width so we can\n",
    "    # generate indices to extract the spans from the sequence tensor.\n",
    "    # These indices will then get masked below, such that if the length\n",
    "    # of a given span is smaller than the max, the rest of the values\n",
    "    # are masked.\n",
    "    max_batch_span_width = span_widths.max().item() + 1\n",
    "\n",
    "    # Shape: (1, 1, max_batch_span_width)\n",
    "    max_span_range_indices = util.get_range_vector(max_batch_span_width, util.get_device_of(target)).view(\n",
    "        1, 1, -1\n",
    "    )\n",
    "#     print(max_batch_span_width)\n",
    "#     print(max_span_range_indices)\n",
    "    # Shape: (batch_size, num_spans, max_batch_span_width)\n",
    "    # This is a broadcasted comparison - for each span we are considering,\n",
    "    # we are creating a range vector of size max_span_width, but masking values\n",
    "    # which are greater than the actual length of the span.\n",
    "    #\n",
    "    # We're using <= here (and for the mask below) because the span ends are\n",
    "    # inclusive, so we want to include indices which are equal to span_widths rather\n",
    "    # than using it as a non-inclusive upper bound.\n",
    "    span_mask = max_span_range_indices <= span_widths\n",
    "#     raw_span_indices = span_ends - max_span_range_indices\n",
    "    raw_span_indices = span_starts + max_span_range_indices\n",
    "#     print(raw_span_indices)\n",
    "#     print(target.size())\n",
    "    # We also don't want to include span indices which are less than zero,\n",
    "    # which happens because some spans near the beginning of the sequence\n",
    "    # have an end index < max_batch_span_width, so we add this to the mask here.\n",
    "    span_mask = span_mask & (raw_span_indices < target.size(1))\n",
    "#     print(span_mask)\n",
    "#     span_indices = torch.nn.functional.relu(raw_span_indices.float()).long()\n",
    "    span_indices = raw_span_indices * span_mask\n",
    "#     print(span_indices)\n",
    "    \n",
    "    # Shape: (batch_size, num_spans, max_batch_span_width, embedding_dim)\n",
    "    span_embeddings = util.batched_index_select(target, span_indices)\n",
    "\n",
    "    return span_embeddings, span_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0],\n",
      "         [ 1],\n",
      "         [ 2],\n",
      "         [ 3],\n",
      "         [ 4],\n",
      "         [ 5],\n",
      "         [ 6],\n",
      "         [ 7],\n",
      "         [ 8],\n",
      "         [ 9],\n",
      "         [10],\n",
      "         [11],\n",
      "         [12],\n",
      "         [13],\n",
      "         [14]],\n",
      "\n",
      "        [[15],\n",
      "         [16],\n",
      "         [17],\n",
      "         [18],\n",
      "         [19],\n",
      "         [20],\n",
      "         [21],\n",
      "         [22],\n",
      "         [23],\n",
      "         [24],\n",
      "         [25],\n",
      "         [26],\n",
      "         [27],\n",
      "         [28],\n",
      "         [29]],\n",
      "\n",
      "        [[30],\n",
      "         [31],\n",
      "         [32],\n",
      "         [33],\n",
      "         [34],\n",
      "         [35],\n",
      "         [36],\n",
      "         [37],\n",
      "         [38],\n",
      "         [39],\n",
      "         [40],\n",
      "         [41],\n",
      "         [42],\n",
      "         [43],\n",
      "         [44]]])\n",
      "tensor([[0, 1],\n",
      "        [2, 4]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# torch.tensor\n",
    "a = util.get_range_vector(45,-1).reshape([3,15,1])\n",
    "print(a)\n",
    "q = 2\n",
    "t = 3\n",
    "b = torch.tensor([[0,q-1],[q,q+t-1]])\n",
    "# b = torch.tensor([[[1,3]],[[2,3]],[[3,4]]])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0],\n",
       "           [1],\n",
       "           [0]],\n",
       " \n",
       "          [[2],\n",
       "           [3],\n",
       "           [4]]]]),\n",
       " tensor([[[ True,  True, False],\n",
       "          [ True,  True,  True]]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_span_select(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[1],\n",
       "           [0],\n",
       "           [0]],\n",
       " \n",
       "          [[4],\n",
       "           [3],\n",
       "           [2]]]]),\n",
       " tensor([[[ True,  True, False],\n",
       "          [ True,  True,  True]]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.batched_span_select(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'fields'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-85339ef0d6ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# list()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'enc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lengths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'fields'"
     ]
    }
   ],
   "source": [
    "# list()\n",
    "import numpy as np\n",
    "b = a.fields['enc']\n",
    "token_ids = [0]+b.tokens\n",
    "c = a.fields['lengths'].array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0]+token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = b.tokens[0]\n",
    "# vars(y)\n",
    "y.text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(c.array)\n",
    "offsets = get_offsets(c)\n",
    "res = [token_ids[offsets[j][0]:offsets[j][1] + 1] for j in range(len(offsets))]\n",
    "# print(res)\n",
    "# print()\n",
    "# for x in [[vocab.get_token_from_index(y,\"tags\") for y in x] for x in res]:\n",
    "for x in res:\n",
    "    print([y.text_id for y in x])\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([1, 205, 2])\n",
    "torch.Size([1, 205, 768])\n",
    "tensor([[ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 16, 13, 12, 12, 12, 14, 14,\n",
    "         13, 12, 13, 12, 13, 13, 13,  4,  4,  4]], dtype=torch.int32) torch.Size([1, 28])\n",
    "tensor([2129])\n",
    "tensor([2116])\n",
    "tensor([4641])\n",
    "tensor([1997])\n",
    "tensor([1996])\n",
    "tensor([7640])\n",
    "tensor([2024])\n",
    "tensor([3080])\n",
    "tensor([2084])\n",
    "tensor([5179])\n",
    "tensor([1029])\n",
    "tensor([ 1026,  2828,  1024,  3793,  1028,  1008,  1026,  2795,  1011, 19802,\n",
    "         1028,  1026,  2151,  1011,  2795,  1028])\n",
    "tensor([ 1026,  2828,  1024,  2193,  1028,  2533,  8909,  1026,  2795,  1011,\n",
    "        19802,  1028,  2533])\n",
    "tensor([ 1026,  2828,  1024,  3793,  1028,  2171,  1026,  2795,  1011, 19802,\n",
    "         1028,  2533])\n",
    "tensor([ 1026,  2828,  1024,  3793,  1028,  4325,  1026,  2795,  1011, 19802,\n",
    "         1028,  2533])\n",
    "tensor([ 1026,  2828,  1024,  2193,  1028,  5464,  1026,  2795,  1011, 19802,\n",
    "         1028,  2533])\n",
    "tensor([ 1026,  2828,  1024,  2193,  1028,  5166,  1999, 25501,  1026,  2795,\n",
    "         1011, 19802,  1028,  2533])\n",
    "tensor([ 1026,  2828,  1024,  2193,  1028, 16371,  1001,  1001,  1049,  5126,\n",
    "         1026,  2795,  1011, 19802])\n",
    "tensor([ 1028,  2533,  1026,  2828,  1024,  2193,  1028,  2132,  8909,  1026,\n",
    "         2795,  1011, 19802])\n",
    "tensor([ 1028,  2132,  1026,  2828,  1024,  3793,  1028,  2171,  1026,  2795,\n",
    "         1011, 19802])\n",
    "tensor([ 1028,  2132,  1026,  2828,  1024,  3793,  1028,  2141,  2110,  1026,\n",
    "         2795,  1011, 19802])\n",
    "tensor([ 1028,  2132,  1026,  2828,  1024,  2193,  1028,  2287,  1026,  2795,\n",
    "         1011, 19802])\n",
    "tensor([ 1028,  2132,  1026,  2828,  1024,  2193,  1028,  2533,  8909,  1026,\n",
    "         2795,  1011, 19802])\n",
    "tensor([ 1028,  2968,  1026,  2828,  1024,  2193,  1028,  2132,  8909,  1026,\n",
    "         2795,  1011, 19802])\n",
    "tensor([ 1028,  2968,  1026,  2828,  1024,  3793,  1028,  5741,  3772,  1026,\n",
    "         2795,  1011, 19802])\n",
    "tensor([1028, 2968, 1026, 2795])\n",
    "tensor([1028, 2533, 1026, 2795])\n",
    "tensor([1028, 2132, 1026, 2795])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=None\n",
    "for inst in train_dataset:\n",
    "    a = inst\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset)\n",
    "b=a.fields['enc']\n",
    "b.index(vocab)\n",
    "tok_dict = b._indexed_tokens['tokens']\n",
    "print(len(tok_dict['token_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "c=a.fields['lengths'].array\n",
    "token_ids = tok_dict['token_ids']\n",
    "old_offsets = tok_dict['offsets']\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_str = lambda x: [vocab.get_token_from_index(y,\"tags\") for y in x]\n",
    "lengths = np.array(list(c))+1\n",
    "# offsets = np.cumsum(lengths-1)\n",
    "# offsets\n",
    "# print(lengths)\n",
    "\n",
    "# offsets = list(zip(offsets,offsets)\n",
    "# offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_ = \n",
    "# b_=offsets+lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_offsets(lengths):\n",
    "    e = np.cumsum(([0]+list(lengths))[:-1])\n",
    "    return list(zip(e+1,e+np.array(lengths)))\n",
    "\n",
    "offsets = get_offsets(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _token_indexers\n",
    "# to_str = lambda x: vocab.get_token_from_index(x,\"tags\")\n",
    "# print(c.array)\n",
    "res = [token_ids[offsets[j][0]:offsets[j][1] + 1] for j in range(len(offsets))]\n",
    "# print(res)\n",
    "# print()\n",
    "for x in [[vocab.get_token_from_index(y,\"tags\") for y in x] for x in res]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.get_token_from_index(102,'tags')\n",
    "# vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= b._token_indexers['tokens']\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.indices_to_tokens([23],vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward_on_instances([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "i=random.randint(0,len(train_dataset))\n",
    "a=None\n",
    "for inst in train_dataset[i:i+1]:\n",
    "    a = inst\n",
    "    res_list = model.forward_on_instances([inst])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.fields['desc'].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= res_list[0]['initial_state']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= b.get_valid_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.get_valid_actions()\n",
    "b.possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "i=random.randint(0,len(train_dataset))\n",
    "a=None\n",
    "for inst in train_dataset[i:i+1]:\n",
    "#     print(inst)\n",
    "    a=inst\n",
    "# b=a.fields['relation']\n",
    "# b.array\n",
    "b=a.fields['world']\n",
    "c=a.fields['desc']\n",
    "d=a.fields['item'] \n",
    "e = a.fields[\"schema\"]\n",
    "# b\n",
    "# print(a.fields['utterance'])\n",
    "for entity in b.metadata.db_context.knowledge_graph.entities:\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in (c.metadata['columns']+c.metadata['tables']):\n",
    "def normalize_schema_constant(entity):\n",
    "#     print(entity)\n",
    "    col = \"_\".join(entity)\n",
    "    col =  col.split(\"_<table-sep>_\")\n",
    "    if len(col)==1:\n",
    "        return \"_\".join(entity)\n",
    "#         print()\n",
    "    else:\n",
    "        table = col[1]\n",
    "        col = col[0].split(\">_\")[1]\n",
    "        return f\"{table}@{col}\"\n",
    "#         print()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.entity_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.fields['valid_actions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.metadata.schema.columns[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"asdas_adsdas\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dataset_readers.ratsql_spider_bert import *\n",
    "\n",
    "# builder = RelationBuilder()\n",
    "\n",
    "\n",
    "db_path = \"dataset/database\"\n",
    "pre = SpiderEncoderV2Preproc()\n",
    "schemas, eval_foreign_key_maps = pre.load_tables([\"dataset/tables.json\"])\n",
    "# for path in paths:\n",
    "for db_id, schema in tqdm(schemas.items(), desc=\"DB connections\"):\n",
    "    sqlite_path = Path(db_path) / db_id / f\"{db_id}.sqlite\"\n",
    "    source: sqlite3.Connection\n",
    "    with sqlite3.connect(sqlite_path) as source:\n",
    "        dest = sqlite3.connect(':memory:')\n",
    "        dest.row_factory = sqlite3.Row\n",
    "        source.backup(dest)\n",
    "    schema.connection = dest\n",
    "    \n",
    "examples=[]\n",
    "raw_data = json.load(open(\"dataset/train_spider.json\"))\n",
    "for entry in raw_data:\n",
    "    item = SpiderItem(\n",
    "        text=entry['question_toks'],\n",
    "        code=entry['sql'],\n",
    "        schema=schemas[entry['db_id']],\n",
    "        orig=entry,\n",
    "        orig_schema=schemas[entry['db_id']].orig)\n",
    "#     desc = pre.preprocess_item(item,\"train\")\n",
    "    examples.append(item)\n",
    "\n",
    "        # Backup in-memory copies of all the DBs and create the live connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples\n",
    "# \n",
    "import pandas as pd\n",
    "print \n",
    "# \n",
    "import tqdm\n",
    "for item in tqdm.tqdm(examples):\n",
    "    desc = pre.preprocess_item(item,\"train\")\n",
    "    q = desc['question']\n",
    "    q_len = len(q)\n",
    "    t = [x[0] for x in  desc['tables']]\n",
    "    t_len = len(t)\n",
    "    c = [\"_\".join(x) for x in  desc['columns']]\n",
    "    c_len = len(c)\n",
    "    enc = q+c+t\n",
    "    print(enc)\n",
    "    relation = pre.compute_relations(desc,len(enc),q_len,c_len,range(c_len+1),range(t_len+1))\n",
    "#     with np.printoptions(threshold=np.inf):\n",
    "#     print(np.array2string())\n",
    "\n",
    "    arr = pd.DataFrame(relation)\n",
    "#     print()\n",
    "    break\n",
    "# arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in a.items():\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=a.fields['utterance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.sequence_length()\n",
    "b.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=a.fields['schema']\n",
    "c.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.compute_relations(dict(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.to_csv(\"t.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
